diff --git a/kernel/sched/MuQSS.c b/kernel/sched/MuQSS.c
index ab972d0..dd5eb0f 100644
--- a/kernel/sched/MuQSS.c
+++ b/kernel/sched/MuQSS.c
@@ -2843,11 +2843,11 @@ EXPORT_SYMBOL(single_task_running);
 
 unsigned long long nr_context_switches(void)
 {
-	int i;
+	int cpu;
 	unsigned long long sum = 0;
 
-	for_each_possible_cpu(i)
-		sum += cpu_rq(i)->nr_switches;
+	for_each_possible_cpu(cpu)
+		sum += cpu_rq(cpu)->nr_switches;
 
 	return sum;
 }
@@ -2896,10 +2896,10 @@ unsigned long nr_iowait_cpu(int cpu)
 
 unsigned long nr_iowait(void)
 {
-	unsigned long i, sum = 0;
+	unsigned long cpu, sum = 0;
 
-	for_each_possible_cpu(i)
-		sum += nr_iowait_cpu(i);
+	for_each_possible_cpu(cpu)
+		sum += nr_iowait_cpu(cpu);
 
 	return sum;
 }
@@ -6162,7 +6162,7 @@ static void bind_zero(int src_cpu)
 	} while_each_thread(t, p);
 
 	if (bound) {
-		printk(KERN_INFO "Removed affinity for %d processes to cpu %d\n",
+		printk(KERN_INFO "MuQSS removed affinity for %d processes to cpu %d\n",
 		       bound, src_cpu);
 	}
 }
@@ -6193,11 +6193,11 @@ static void unbind_zero(int src_cpu)
 	} while_each_thread(t, p);
 
 	if (unbound) {
-		printk(KERN_INFO "Added affinity for %d processes to cpu %d\n",
+		printk(KERN_INFO "MuQSS added affinity for %d processes to cpu %d\n",
 		       unbound, src_cpu);
 	}
 	if (zerobound) {
-		printk(KERN_INFO "Released forced binding to cpu0 for %d processes\n",
+		printk(KERN_INFO "MuQSS released forced binding to cpu0 for %d processes\n",
 		       zerobound);
 	}
 }
@@ -6689,6 +6689,10 @@ void __init sched_init_smp(void)
 	local_irq_disable();
 	mutex_lock(&sched_domains_mutex);
 	lock_all_rqs();
+
+	printk(KERN_INFO "MuQSS possible/present/online CPUs: %d/%d/%d\n",
+		num_possible_cpus(), num_present_cpus(), num_online_cpus());
+
 	/*
 	 * Set up the relative cache distance of each online cpu from each
 	 * other in a simple array for quick lookup. Locality is determined
@@ -6782,10 +6786,7 @@ void __init sched_init_smp(void)
 
 	for_each_online_cpu(cpu) {
 		rq = cpu_rq(cpu);
-
 		for_each_online_cpu(other_cpu) {
-			if (other_cpu <= cpu)
-				continue;
 			printk(KERN_DEBUG "MuQSS locality CPU %d to %d: %d\n", cpu, other_cpu, rq->cpu_locality[other_cpu]);
 		}
 	}
@@ -6796,7 +6797,7 @@ void __init sched_init_smp(void)
 
 		rq_lock(rq);
 		if (leader && rq != leader) {
-			printk(KERN_INFO "Sharing SMP runqueue from CPU %d to CPU %d\n",
+			printk(KERN_INFO "MuQSS sharing SMP runqueue from CPU %d to CPU %d\n",
 			       leader->cpu, rq->cpu);
 			kfree(rq->node);
 			kfree(rq->sl);
@@ -6818,7 +6819,7 @@ void __init sched_init_smp(void)
 
 		rq_lock(rq);
 		if (leader && rq != leader) {
-			printk(KERN_INFO "Sharing MC runqueue from CPU %d to CPU %d\n",
+			printk(KERN_INFO "MuQSS sharing MC runqueue from CPU %d to CPU %d\n",
 			       leader->cpu, rq->cpu);
 			kfree(rq->node);
 			kfree(rq->sl);
@@ -6842,7 +6843,7 @@ void __init sched_init_smp(void)
 
 		rq_lock(rq);
 		if (leader && rq != leader) {
-			printk(KERN_INFO "Sharing SMT runqueue from CPU %d to CPU %d\n",
+			printk(KERN_INFO "MuQSS sharing SMT runqueue from CPU %d to CPU %d\n",
 			       leader->cpu, rq->cpu);
 			kfree(rq->node);
 			kfree(rq->sl);
@@ -6861,7 +6862,7 @@ void __init sched_init_smp(void)
 	local_irq_enable();
 
 	total_runqueues = 0;
-	for_each_possible_cpu(cpu) {
+	for_each_online_cpu(cpu) {
 		int locality, total_rqs = 0, total_cpus = 0;
 
 		rq = cpu_rq(cpu);
@@ -6878,18 +6879,18 @@ void __init sched_init_smp(void)
 		for (locality = 0; locality <= 4; locality++) {
 			int test_cpu;
 
-			for_each_possible_cpu(test_cpu) {
+			for_each_online_cpu(test_cpu) {
 				/* Work from each CPU up instead of every rq
 				 * starting at CPU 0. Orders are better matched
 				 * if the top half CPUs count down instead. */
-				if (cpu < num_possible_cpus() / 2)
+				if (cpu < num_online_cpus() / 2)
 					other_cpu = cpu + test_cpu;
 				else
 					other_cpu = cpu - test_cpu;
 				if (other_cpu < 0)
-					other_cpu += num_possible_cpus();
+					other_cpu += num_online_cpus();
 				else
-					other_cpu %= num_possible_cpus();
+					other_cpu %= num_online_cpus();
 				other_rq = cpu_rq(other_cpu);
 
 				if (rq->cpu_locality[other_cpu] == locality) {
@@ -6909,20 +6910,22 @@ void __init sched_init_smp(void)
 		}
 	}
 
-	for_each_possible_cpu(cpu) {
+	for_each_online_cpu(cpu) {
 		rq = cpu_rq(cpu);
 		for (i = 0; i < total_runqueues; i++) {
-			printk(KERN_DEBUG "CPU %d RQ order %d RQ %d\n", cpu, i,
+			printk(KERN_DEBUG "MuQSS CPU %d RQ order %d RQ %d\n", cpu, i,
 			       rq->rq_order[i]->cpu);
 		}
 	}
-	for_each_possible_cpu(cpu) {
+
+	for_each_online_cpu(cpu) {
 		rq = cpu_rq(cpu);
-		for (i = 0; i < num_possible_cpus(); i++) {
-			printk(KERN_DEBUG "CPU %d CPU order %d RQ %d\n", cpu, i,
+		for (i = 0; i < num_online_cpus(); i++) {
+			printk(KERN_DEBUG "MuQSS CPU %d CPU order %d RQ %d\n", cpu, i,
 			       rq->cpu_order[i]->cpu);
 		}
 	}
+
 	switch (rqshare) {
 		case RQSHARE_ALL:
 			/* This should only ever read 1 */
