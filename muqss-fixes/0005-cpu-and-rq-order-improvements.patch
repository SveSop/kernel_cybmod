diff --git a/kernel/sched/MuQSS.c b/kernel/sched/MuQSS.c
index f202622..3d2d788 100644
--- a/kernel/sched/MuQSS.c
+++ b/kernel/sched/MuQSS.c
@@ -6901,21 +6901,22 @@ void __init sched_init_smp(void)
 		rq = cpu_rq(cpu);
 		if (
 #ifdef CONFIG_SCHED_MC
-		    (rq->mc_leader == rq) &&
+			(rq->mc_leader == rq) &&
 #endif
 #ifdef CONFIG_SCHED_SMT
-		    (rq->smt_leader == rq) &&
+			(rq->smt_leader == rq) &&
 #endif
-	            (rq->smp_leader == rq))
+			(rq->smp_leader == rq))
 			total_runqueues++;
 
 		for (locality = LOCALITY_SAME; locality <= LOCALITY_DISTANT; locality++) {
-			int test_cpu;
+			int selected_cpus[NR_CPUS], selected_cpu_cnt, selected_cpu_idx, test_cpu_idx, cpu_idx, best_locality, test_cpu;
+			int ordered_cpus[NR_CPUS], ordered_cpus_idx;
+
+			ordered_cpus_idx = -1;
+			selected_cpu_cnt = 0;
 
 			for_each_online_cpu(test_cpu) {
-				/* Work from each CPU up instead of every rq
-				 * starting at CPU 0. Orders are better matched
-				 * if the top half CPUs count down instead. */
 				if (cpu < num_online_cpus() / 2)
 					other_cpu = cpu + test_cpu;
 				else
@@ -6924,20 +6925,65 @@ void __init sched_init_smp(void)
 					other_cpu += num_online_cpus();
 				else
 					other_cpu %= num_online_cpus();
-				other_rq = cpu_rq(other_cpu);
-
+				/* gather CPUs of the same locality */
 				if (rq->cpu_locality[other_cpu] == locality) {
-					rq->cpu_order[total_cpus++] = other_rq;
-					if (
+					selected_cpus[selected_cpu_cnt] = other_cpu;
+					selected_cpu_cnt++;
+				}
+			}
+
+			/* reserve first CPU as starting point */
+			if (selected_cpu_cnt > 0) {
+				ordered_cpus_idx++;
+				ordered_cpus[ordered_cpus_idx] = selected_cpus[ordered_cpus_idx];
+				selected_cpus[ordered_cpus_idx] = -1;
+			}
+
+			/* take each CPU and sort it within the same locality based on each inter-CPU localities */
+			for(test_cpu_idx = 1; test_cpu_idx < selected_cpu_cnt; test_cpu_idx++) {
+				/* starting point with worst locality and current CPU */
+				best_locality = LOCALITY_DISTANT;
+				selected_cpu_idx = test_cpu_idx;
+
+				/* try to find the best locality within group */
+				for(cpu_idx = 1; cpu_idx < selected_cpu_cnt; cpu_idx++)
+				{
+					/* if CPU has not been used and locality is better */
+					if (selected_cpus[cpu_idx] > -1)
+					{
+						other_rq = cpu_rq(ordered_cpus[ordered_cpus_idx]);
+						if (best_locality > other_rq->cpu_locality[selected_cpus[cpu_idx]])
+						{
+							/* assign best loclity and best CPU idx in array */
+							best_locality = other_rq->cpu_locality[selected_cpus[cpu_idx]];
+							selected_cpu_idx = cpu_idx;
+						}
+					}
+				}
+
+				/* add our next best CPU to ordered list */
+				ordered_cpus_idx++;
+				ordered_cpus[ordered_cpus_idx] = selected_cpus[selected_cpu_idx];
+				/* mark this CPU as used */
+				selected_cpus[selected_cpu_idx] =  -1;
+			}
+
+			/* set up RQ and CPU orders */
+			for (test_cpu = 0; test_cpu <= ordered_cpus_idx; test_cpu++)
+			{
+				other_rq = cpu_rq(ordered_cpus[test_cpu]);
+				/* set up cpu orders */
+				rq->cpu_order[total_cpus++] = other_rq;
+				if (
 #ifdef CONFIG_SCHED_MC
-					    (other_rq->mc_leader == other_rq) &&
+					(other_rq->mc_leader == other_rq) &&
 #endif
 #ifdef CONFIG_SCHED_SMT
-					    (other_rq->smt_leader == other_rq) &&
+					(other_rq->smt_leader == other_rq) &&
 #endif
-					    (other_rq->smp_leader == other_rq))
-						rq->rq_order[total_rqs++] = other_rq;
-				}
+					(other_rq->smp_leader == other_rq))
+					/* set up RQ orders */
+					rq->rq_order[total_rqs++] = other_rq;
 			}
 		}
 	}
