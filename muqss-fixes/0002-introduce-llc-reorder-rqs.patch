diff --git a/kernel/sched/MuQSS.c b/kernel/sched/MuQSS.c
index dd5eb0f..49b8a9a 100644
--- a/kernel/sched/MuQSS.c
+++ b/kernel/sched/MuQSS.c
@@ -29,6 +29,7 @@
  *              a whole lot of those previous things.
  *  2016-10-01  Multiple Queue Skiplist Scheduler scalable evolution of BFS
  * 		scheduler by Con Kolivas.
+ *  2019-08-31  LLC bits by Eduards Bezverhijs
  */
 
 #include <linux/sched/isolation.h>
@@ -114,12 +115,21 @@ void print_scheduler_version(void)
 	printk(KERN_INFO "MuQSS CPU scheduler v0.193 by Con Kolivas.\n");
 }
 
+/* Define RQ share levels */
 #define RQSHARE_NONE 0
 #define RQSHARE_SMT 1
 #define RQSHARE_MC 2
 #define RQSHARE_SMP 3
 #define RQSHARE_ALL 4
 
+/* Define locality levels */
+#define LOCALITY_SAME 0
+#define LOCALITY_SMT 1
+#define LOCALITY_MC_LLC 2
+#define LOCALITY_MC 3
+#define LOCALITY_SMP 4
+#define LOCALITY_DISTANT 5
+
 /*
  * This determines what level of runqueue sharing will be done and is
  * configurable at boot time with the bootparam rqshare =
@@ -1049,12 +1059,13 @@ static void resched_curr(struct rq *rq)
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
-#define CPUIDLE_DIFF_THREAD	(1)
-#define CPUIDLE_DIFF_CORE	(2)
-#define CPUIDLE_CACHE_BUSY	(4)
-#define CPUIDLE_DIFF_CPU	(8)
-#define CPUIDLE_THREAD_BUSY	(16)
-#define CPUIDLE_DIFF_NODE	(32)
+#define CPUIDLE_DIFF_THREAD     (1)
+#define CPUIDLE_DIFF_CORE_LLC   (2)
+#define CPUIDLE_DIFF_CORE       (4)
+#define CPUIDLE_CACHE_BUSY      (8)
+#define CPUIDLE_DIFF_CPU        (16)
+#define CPUIDLE_THREAD_BUSY     (32)
+#define CPUIDLE_DIFF_NODE       (64)
 
 /*
  * The best idle CPU is chosen according to the CPUIDLE ranking above where the
@@ -1075,7 +1086,7 @@ static int best_mask_cpu(int best_cpu, struct rq *rq, cpumask_t *tmpmask)
 {
 	int best_ranking = CPUIDLE_DIFF_NODE | CPUIDLE_THREAD_BUSY |
 		CPUIDLE_DIFF_CPU | CPUIDLE_CACHE_BUSY | CPUIDLE_DIFF_CORE |
-		CPUIDLE_DIFF_THREAD;
+		CPUIDLE_DIFF_CORE_LLC | CPUIDLE_DIFF_THREAD;
 	int cpu_tmp;
 
 	if (cpumask_test_cpu(best_cpu, tmpmask))
@@ -1090,20 +1101,22 @@ static int best_mask_cpu(int best_cpu, struct rq *rq, cpumask_t *tmpmask)
 
 		locality = rq->cpu_locality[cpu_tmp];
 #ifdef CONFIG_NUMA
-		if (locality > 3)
+		if (locality > LOCALITY_SMP)
 			ranking |= CPUIDLE_DIFF_NODE;
 		else
 #endif
-		if (locality > 2)
+		if (locality > LOCALITY_MC)
 			ranking |= CPUIDLE_DIFF_CPU;
 #ifdef CONFIG_SCHED_MC
-		else if (locality == 2)
+		else if (locality == LOCALITY_MC_LLC)
+			ranking |= CPUIDLE_DIFF_CORE_LLC;
+		else if (locality == LOCALITY_MC)
 			ranking |= CPUIDLE_DIFF_CORE;
 		else if (!(tmp_rq->cache_idle(tmp_rq)))
 			ranking |= CPUIDLE_CACHE_BUSY;
 #endif
 #ifdef CONFIG_SCHED_SMT
-		if (locality == 1)
+		if (locality == LOCALITY_SMT)
 			ranking |= CPUIDLE_DIFF_THREAD;
 		if (!(tmp_rq->siblings_idle(tmp_rq)))
 			ranking |= CPUIDLE_THREAD_BUSY;
@@ -1121,7 +1134,7 @@ bool cpus_share_cache(int this_cpu, int that_cpu)
 {
 	struct rq *this_rq = cpu_rq(this_cpu);
 
-	return (this_rq->cpu_locality[that_cpu] < 3);
+	return (this_rq->cpu_locality[that_cpu] < LOCALITY_SMP);
 }
 
 /* As per resched_curr but only will resched idle task */
@@ -3658,7 +3671,7 @@ static inline struct task_struct
 			 * Don't reschedule balance across nodes unless the CPU
 			 * is idle.
 			 */
-			if (edt != idle && rq->cpu_locality[other_rq->cpu] > 3)
+			if (edt != idle && rq->cpu_locality[other_rq->cpu] > LOCALITY_SMP)
 				break;
 			if (entries <= best_entries)
 				continue;
@@ -6702,9 +6715,9 @@ void __init sched_init_smp(void)
 	 * treated as not local. CPUs not even in the same domain (different
 	 * nodes) are treated as very distant.
 	 */
-	for_each_online_cpu(cpu) {
+	for (cpu = num_online_cpus() - 1; cpu >= 0; cpu--) {
 		rq = cpu_rq(cpu);
-
+		leader = NULL;
 		/* First check if this cpu is in the same node */
 		for_each_domain(cpu, sd) {
 			if (sd->level > SD_LV_MC)
@@ -6722,8 +6735,8 @@ void __init sched_init_smp(void)
 					other_rq->smp_leader = leader;
 				}
 
-				if (rq->cpu_locality[other_cpu] > 3)
-					rq->cpu_locality[other_cpu] = 3;
+				if (rq->cpu_locality[other_cpu] > LOCALITY_SMP)
+					rq->cpu_locality[other_cpu] = LOCALITY_SMP;
 			}
 		}
 
@@ -6745,8 +6758,13 @@ void __init sched_init_smp(void)
 						leader = rq;
 					other_rq->mc_leader = leader;
 				}
-				if (rq->cpu_locality[other_cpu] > 2)
-					rq->cpu_locality[other_cpu] = 2;
+				if (rq->cpu_locality[other_cpu] > LOCALITY_MC) {
+					/* this is to get LLC into play */
+					if (per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, other_cpu))
+						rq->cpu_locality[other_cpu] = LOCALITY_MC_LLC;
+					else
+						rq->cpu_locality[other_cpu] = LOCALITY_MC;
+				}
 			}
 			rq->cache_idle = cache_cpu_idle;
 		}
@@ -6765,8 +6783,8 @@ void __init sched_init_smp(void)
 						leader = rq;
 					other_rq->smt_leader = leader;
 				}
-				if (rq->cpu_locality[other_cpu] > 1)
-					rq->cpu_locality[other_cpu] = 1;
+				if (rq->cpu_locality[other_cpu] > LOCALITY_SMT)
+					rq->cpu_locality[other_cpu] = LOCALITY_SMT;
 			}
 			rq->siblings_idle = siblings_cpu_idle;
 			smt_threads = true;
@@ -6876,7 +6894,7 @@ void __init sched_init_smp(void)
 	            (rq->smp_leader == rq))
 			total_runqueues++;
 
-		for (locality = 0; locality <= 4; locality++) {
+		for (locality = LOCALITY_SAME; locality <= LOCALITY_DISTANT; locality++) {
 			int test_cpu;
 
 			for_each_online_cpu(test_cpu) {
@@ -6896,7 +6914,6 @@ void __init sched_init_smp(void)
 				if (rq->cpu_locality[other_cpu] == locality) {
 					rq->cpu_order[total_cpus++] = other_rq;
 					if (
-
 #ifdef CONFIG_SCHED_MC
 					    (other_rq->mc_leader == other_rq) &&
 #endif
@@ -6913,16 +6930,16 @@ void __init sched_init_smp(void)
 	for_each_online_cpu(cpu) {
 		rq = cpu_rq(cpu);
 		for (i = 0; i < total_runqueues; i++) {
-			printk(KERN_DEBUG "MuQSS CPU %d RQ order %d RQ %d\n", cpu, i,
-			       rq->rq_order[i]->cpu);
+			printk(KERN_DEBUG "MuQSS CPU %d llc %d RQ order %d RQ %d llc %d\n", cpu, per_cpu(cpu_llc_id, cpu), i,
+			       rq->rq_order[i]->cpu, per_cpu(cpu_llc_id, rq->rq_order[i]->cpu));
 		}
 	}
 
 	for_each_online_cpu(cpu) {
 		rq = cpu_rq(cpu);
 		for (i = 0; i < num_online_cpus(); i++) {
-			printk(KERN_DEBUG "MuQSS CPU %d CPU order %d RQ %d\n", cpu, i,
-			       rq->cpu_order[i]->cpu);
+			printk(KERN_DEBUG "MuQSS CPU %d llc %d CPU order %d RQ %d llc %d\n", cpu, per_cpu(cpu_llc_id, cpu), i,
+			       rq->cpu_order[i]->cpu, per_cpu(cpu_llc_id, rq->cpu_order[i]->cpu));
 		}
 	}
 
@@ -7075,9 +7092,9 @@ void __init sched_init(void)
 		rq->cpu_locality = kmalloc(cpu_ids * sizeof(int *), GFP_ATOMIC);
 		for_each_possible_cpu(j) {
 			if (i == j)
-				rq->cpu_locality[j] = 0;
+				rq->cpu_locality[j] = LOCALITY_SAME;
 			else
-				rq->cpu_locality[j] = 4;
+				rq->cpu_locality[j] = LOCALITY_DISTANT;
 		}
 		rq->rq_order = kmalloc(cpu_ids * sizeof(struct rq *), GFP_ATOMIC);
 		rq->cpu_order = kmalloc(cpu_ids * sizeof(struct rq *), GFP_ATOMIC);
