From 01c596b64ef06f064a2a2e83a40cc252135a05ab Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@redhat.com>
Date: Wed, 29 Apr 2020 09:03:42 +0200
Subject: [PATCH 09/10] mm-5.6: update proactive compaction to v4

Signed-off-by: Oleksandr Natalenko <oleksandr@redhat.com>
---
 .../admin-guide/mm/proactive-compaction.rst   | 26 ++++++++
 MAINTAINERS                                   |  6 ++
 mm/compaction.c                               | 63 +++++++++++++++----
 mm/vmstat.c                                   |  5 ++
 4 files changed, 89 insertions(+), 11 deletions(-)
 create mode 100644 Documentation/admin-guide/mm/proactive-compaction.rst

diff --git a/Documentation/admin-guide/mm/proactive-compaction.rst b/Documentation/admin-guide/mm/proactive-compaction.rst
new file mode 100644
index 000000000..510f47e38
--- /dev/null
+++ b/Documentation/admin-guide/mm/proactive-compaction.rst
@@ -0,0 +1,26 @@
+.. SPDX-License-Identifier: GPL-2.0
+.. _proactive_compaction:
+
+====================
+Proactive Compaction
+====================
+
+Many applications benefit significantly from the use of huge pages.
+However, huge-page allocations often incur a high latency or even fail
+under fragmented memory conditions. Proactive compaction provides an
+effective solution to these problems by doing memory compaction in the
+background.
+
+The process of proactive compaction is controlled by a single tunable:
+
+        /sys/kernel/mm/compaction/proactiveness
+
+This tunable takes a value in the range [0, 100] with a default value of
+20. This tunable determines how aggressively compaction is done in the
+background. Setting it to 0 disables proactive compaction.
+
+Note that compaction has a non-trivial system-wide impact as pages
+belonging to different processes are moved around, which could also lead
+to latency spikes in unsuspecting applications. The kernel employs
+various heuristics to avoid wasting CPU cycles if it detects that
+proactive compaction is not being effective.
diff --git a/MAINTAINERS b/MAINTAINERS
index 5a5332b35..1fdb5f1b4 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -18547,6 +18547,12 @@ L:	linux-mm@kvack.org
 S:	Maintained
 F:	mm/zswap.c
 
+PROACTIVE COMPACTION
+M:	Nitin Gupta <nigupta@nvidia.com>
+L:	linux-mm@kvack.org
+S:	Maintained
+F:	Documentation/admin-guide/mm/proactive-compaction.rst
+
 THE REST
 M:	Linus Torvalds <torvalds@linux-foundation.org>
 L:	linux-kernel@vger.kernel.org
diff --git a/mm/compaction.c b/mm/compaction.c
index 0344315ae..3d299404d 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -26,6 +26,14 @@
 #include "internal.h"
 
 #ifdef CONFIG_COMPACTION
+/*
+ * Tunable for proactive compaction, exposed via sysfs:
+ *	/sys/kernel/mm/compaction/proactiveness
+ *
+ * This tunable determines how aggressively the kernel
+ * should compact memory in the background. It takes
+ * values in the range [0, 100].
+ */
 static unsigned int compaction_proactiveness = 20;
 
 static inline void count_compact_event(enum vm_event_item item)
@@ -52,6 +60,9 @@ static inline void count_compact_events(enum vm_event_item item, long delta)
 #define pageblock_start_pfn(pfn)	block_start_pfn(pfn, pageblock_order)
 #define pageblock_end_pfn(pfn)		block_end_pfn(pfn, pageblock_order)
 
+/*
+ * Fragmentation score check interval for proactive compaction purposes.
+ */
 static const int HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500;
 
 static unsigned long release_freepages(struct list_head *freelist)
@@ -1855,7 +1866,17 @@ static bool kswapd_is_running(pg_data_t *pgdat)
 	return pgdat->kswapd && (pgdat->kswapd->state == TASK_RUNNING);
 }
 
-static int proactive_compaction_score_zone(struct zone *zone)
+/*
+ * A zone's fragmentation score is the external fragmentation wrt to the
+ * HUGETLB_PAGE_ORDER scaled by the zone's size. It returns a value in the
+ * range [0, 100].
+
+ * The scaling factor ensures that proactive compaction focuses on larger
+ * zones like ZONE_NORMAL, rather than smaller, specialized zones like
+ * ZONE_DMA32. For smaller zones, the score value remains close to zero,
+ * and thus never exceeds the high threshold for proactive compaction.
+ */
+static int fragmentation_score_zone(struct zone *zone)
 {
 	unsigned long score;
 
@@ -1866,7 +1887,14 @@ static int proactive_compaction_score_zone(struct zone *zone)
 	return score;
 }
 
-static int proactive_compaction_score_node(pg_data_t *pgdat)
+/*
+ * The per-node proactive (background) compaction process is started by its
+ * corresponding kcompactd thread when the node's fragmentation score
+ * exceeds the high threshold. The compaction process remains active till
+ * the node's score falls below the low threshold, or one of the back-off
+ * conditions is met.
+ */
+static int fragmentation_score_node(pg_data_t *pgdat)
 {
 	unsigned long score = 0;
 	int zoneid;
@@ -1875,13 +1903,13 @@ static int proactive_compaction_score_node(pg_data_t *pgdat)
 		struct zone *zone;
 
 		zone = &pgdat->node_zones[zoneid];
-		score += proactive_compaction_score_zone(zone);
+		score += fragmentation_score_zone(zone);
 	}
 
 	return score;
 }
 
-static int proactive_compaction_score_wmark(pg_data_t *pgdat, bool low)
+static int fragmentation_score_wmark(pg_data_t *pgdat, bool low)
 {
 	int wmark_low;
 
@@ -1896,8 +1924,8 @@ static bool should_proactive_compact_node(pg_data_t *pgdat)
 	if (!compaction_proactiveness || kswapd_is_running(pgdat))
 		return false;
 
-	wmark_high = proactive_compaction_score_wmark(pgdat, false);
-	return proactive_compaction_score_node(pgdat) > wmark_high;
+	wmark_high = fragmentation_score_wmark(pgdat, false);
+	return fragmentation_score_node(pgdat) > wmark_high;
 }
 
 static enum compact_result __compact_finished(struct compact_control *cc)
@@ -1934,8 +1962,8 @@ static enum compact_result __compact_finished(struct compact_control *cc)
 		if (kswapd_is_running(pgdat))
 			return COMPACT_PARTIAL_SKIPPED;
 
-		score = proactive_compaction_score_zone(cc->zone);
-		wmark_low = proactive_compaction_score_wmark(pgdat, true);
+		score = fragmentation_score_zone(cc->zone);
+		wmark_low = fragmentation_score_wmark(pgdat, true);
 
 		if (score > wmark_low)
 			ret = COMPACT_CONTINUE;
@@ -2479,7 +2507,15 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,
 	return rc;
 }
 
-/* Compact all zones within a node according to proactiveness */
+/*
+ * Compact all zones within a node till each zone's fragmentation score
+ * reaches within proactive compaction thresholds (as determined by the
+ * proactiveness tunable).
+ *
+ * It is possible that the function returns before reaching score targets
+ * due to various back-off conditions, such as, contention on per-node or
+ * per-zone locks.
+ */
 static void proactive_compact_node(pg_data_t *pgdat)
 {
 	int zoneid;
@@ -2818,6 +2854,7 @@ static int kcompactd(void *p)
 			continue;
 		}
 
+		/* kcompactd wait timeout */
 		if (should_proactive_compact_node(pgdat)) {
 			unsigned int prev_score, score;
 
@@ -2825,9 +2862,13 @@ static int kcompactd(void *p)
 				proactive_defer--;
 				continue;
 			}
-			prev_score = proactive_compaction_score_node(pgdat);
+			prev_score = fragmentation_score_node(pgdat);
 			proactive_compact_node(pgdat);
-			score = proactive_compaction_score_node(pgdat);
+			score = fragmentation_score_node(pgdat);
+			/*
+			 * Defer proactive compaction if the fragmentation
+			 * score did not go down i.e. no progress made.
+			 */
 			proactive_defer = score < prev_score ?
 					0 : 1 << COMPACT_MAX_DEFER_SHIFT;
 		}
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 70d724122..825aae086 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1074,6 +1074,11 @@ static int __fragmentation_index(unsigned int order, struct contig_page_info *in
 	return 1000 - div_u64( (1000+(div_u64(info->free_pages * 1000ULL, requested))), info->free_blocks_total);
 }
 
+/*
+ * Calculates external fragmentation within a zone wrt the given order.
+ * It is defined as the percentage of pages found in blocks of size
+ * less than 1 << order. It returns values in range [0, 100].
+ */
 int extfrag_for_order(struct zone *zone, unsigned int order)
 {
 	struct contig_page_info info;
-- 
2.26.2.266.ge870325ee8

