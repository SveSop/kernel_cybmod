From 31cbe27630ded5e76aeb566d47c0f3d3ca2133ac Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Tue, 19 May 2020 12:47:59 +0200
Subject: [PATCH 11/14] mm-5.6: update proactive compaction to v5

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 .../admin-guide/mm/proactive-compaction.rst   | 26 ------
 Documentation/admin-guide/sysctl/vm.rst       | 13 +++
 MAINTAINERS                                   |  6 --
 include/linux/compaction.h                    |  1 +
 kernel/sysctl.c                               |  9 ++
 mm/compaction.c                               | 93 +++----------------
 mm/page_alloc.c                               |  1 -
 7 files changed, 35 insertions(+), 114 deletions(-)
 delete mode 100644 Documentation/admin-guide/mm/proactive-compaction.rst

diff --git a/Documentation/admin-guide/mm/proactive-compaction.rst b/Documentation/admin-guide/mm/proactive-compaction.rst
deleted file mode 100644
index 510f47e38..000000000
--- a/Documentation/admin-guide/mm/proactive-compaction.rst
+++ /dev/null
@@ -1,26 +0,0 @@
-.. SPDX-License-Identifier: GPL-2.0
-.. _proactive_compaction:
-
-====================
-Proactive Compaction
-====================
-
-Many applications benefit significantly from the use of huge pages.
-However, huge-page allocations often incur a high latency or even fail
-under fragmented memory conditions. Proactive compaction provides an
-effective solution to these problems by doing memory compaction in the
-background.
-
-The process of proactive compaction is controlled by a single tunable:
-
-        /sys/kernel/mm/compaction/proactiveness
-
-This tunable takes a value in the range [0, 100] with a default value of
-20. This tunable determines how aggressively compaction is done in the
-background. Setting it to 0 disables proactive compaction.
-
-Note that compaction has a non-trivial system-wide impact as pages
-belonging to different processes are moved around, which could also lead
-to latency spikes in unsuspecting applications. The kernel employs
-various heuristics to avoid wasting CPU cycles if it detects that
-proactive compaction is not being effective.
diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index 64aeee100..4b975faec 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -119,6 +119,19 @@ all zones are compacted such that free memory is available in contiguous
 blocks where possible. This can be important for example in the allocation of
 huge pages although processes will also directly compact memory as required.
 
+compaction_proactiveness
+========================
+
+This tunable takes a value in the range [0, 100] with a default value of
+20. This tunable determines how aggressively compaction is done in the
+background. Setting it to 0 disables proactive compaction.
+
+Note that compaction has a non-trivial system-wide impact as pages
+belonging to different processes are moved around, which could also lead
+to latency spikes in unsuspecting applications. The kernel employs
+various heuristics to avoid wasting CPU cycles if it detects that
+proactive compaction is not being effective.
+
 
 compact_unevictable_allowed
 ===========================
diff --git a/MAINTAINERS b/MAINTAINERS
index 1fdb5f1b4..5a5332b35 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -18547,12 +18547,6 @@ L:	linux-mm@kvack.org
 S:	Maintained
 F:	mm/zswap.c
 
-PROACTIVE COMPACTION
-M:	Nitin Gupta <nigupta@nvidia.com>
-L:	linux-mm@kvack.org
-S:	Maintained
-F:	Documentation/admin-guide/mm/proactive-compaction.rst
-
 THE REST
 M:	Linus Torvalds <torvalds@linux-foundation.org>
 L:	linux-kernel@vger.kernel.org
diff --git a/include/linux/compaction.h b/include/linux/compaction.h
index 47c5df1fa..ccd28978b 100644
--- a/include/linux/compaction.h
+++ b/include/linux/compaction.h
@@ -85,6 +85,7 @@ static inline unsigned long compact_gap(unsigned int order)
 
 #ifdef CONFIG_COMPACTION
 extern int sysctl_compact_memory;
+extern int sysctl_compaction_proactiveness;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos);
 extern int sysctl_extfrag_threshold;
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index ad5b88a53..ff31b99b9 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1470,6 +1470,15 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0200,
 		.proc_handler	= sysctl_compaction_handler,
 	},
+	{
+		.procname	= "compaction_proactiveness",
+		.data		= &sysctl_compaction_proactiveness,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &one_hundred,
+	},
 	{
 		.procname	= "extfrag_threshold",
 		.data		= &sysctl_extfrag_threshold,
diff --git a/mm/compaction.c b/mm/compaction.c
index 3d299404d..40fa0c6b0 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -26,16 +26,6 @@
 #include "internal.h"
 
 #ifdef CONFIG_COMPACTION
-/*
- * Tunable for proactive compaction, exposed via sysfs:
- *	/sys/kernel/mm/compaction/proactiveness
- *
- * This tunable determines how aggressively the kernel
- * should compact memory in the background. It takes
- * values in the range [0, 100].
- */
-static unsigned int compaction_proactiveness = 20;
-
 static inline void count_compact_event(enum vm_event_item item)
 {
 	count_vm_event(item);
@@ -1881,10 +1871,8 @@ static int fragmentation_score_zone(struct zone *zone)
 	unsigned long score;
 
 	score = zone->present_pages *
-			extfrag_for_order(zone, HUGETLB_PAGE_ORDER);
-	score = div64_ul(score,
-			node_present_pages(zone->zone_pgdat->node_id) + 1);
-	return score;
+			extfrag_for_order(zone, HPAGE_PMD_ORDER);
+	return div64_ul(score, zone->zone_pgdat->node_present_pages + 1);
 }
 
 /*
@@ -1913,7 +1901,7 @@ static int fragmentation_score_wmark(pg_data_t *pgdat, bool low)
 {
 	int wmark_low;
 
-	wmark_low = 100 - compaction_proactiveness;
+	wmark_low = 100 - sysctl_compaction_proactiveness;
 	return low ? wmark_low : min(wmark_low + 10, 100);
 }
 
@@ -1921,7 +1909,7 @@ static bool should_proactive_compact_node(pg_data_t *pgdat)
 {
 	int wmark_high;
 
-	if (!compaction_proactiveness || kswapd_is_running(pgdat))
+	if (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))
 		return false;
 
 	wmark_high = fragmentation_score_wmark(pgdat, false);
@@ -2403,7 +2391,6 @@ static enum compact_result compact_zone_order(struct zone *zone, int order,
 		.alloc_flags = alloc_flags,
 		.classzone_idx = classzone_idx,
 		.direct_compaction = true,
-		.proactive_compaction = false,
 		.whole_zone = (prio == MIN_COMPACT_PRIORITY),
 		.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),
 		.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)
@@ -2526,7 +2513,6 @@ static void proactive_compact_node(pg_data_t *pgdat)
 		.ignore_skip_hint = true,
 		.whole_zone = true,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
 		.proactive_compaction = true,
 	};
 
@@ -2556,10 +2542,9 @@ static void compact_node(int nid)
 		.ignore_skip_hint = true,
 		.whole_zone = true,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
-		.proactive_compaction = false,
 	};
 
+
 	for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
 
 		zone = &pgdat->node_zones[zoneid];
@@ -2590,6 +2575,13 @@ static void compact_nodes(void)
 /* The written value is actually unused, all memory is compacted */
 int sysctl_compact_memory;
 
+/*
+ * Tunable for proactive compaction. It determines how
+ * aggressively the kernel should compact memory in the
+ * background. It takes values in the range [0, 100].
+ */
+int sysctl_compaction_proactiveness = 20;
+
 /*
  * This is the entry point for compacting all nodes via
  * /proc/sys/vm/compact_memory
@@ -2632,63 +2624,6 @@ void compaction_unregister_node(struct node *node)
 }
 #endif /* CONFIG_SYSFS && CONFIG_NUMA */
 
-#ifdef CONFIG_SYSFS
-
-#define COMPACTION_ATTR_RO(_name) \
-	static struct kobj_attribute _name##_attr = __ATTR_RO(_name)
-
-#define COMPACTION_ATTR(_name) \
-	static struct kobj_attribute _name##_attr = \
-		__ATTR(_name, 0644, _name##_show, _name##_store)
-
-static struct kobject *compaction_kobj;
-
-static ssize_t proactiveness_store(struct kobject *kobj,
-		struct kobj_attribute *attr, const char *buf, size_t count)
-{
-	int err;
-	unsigned long input;
-
-	err = kstrtoul(buf, 10, &input);
-	if (err)
-		return err;
-	if (input > 100)
-		return -EINVAL;
-
-	compaction_proactiveness = input;
-	return count;
-}
-
-static ssize_t proactiveness_show(struct kobject *kobj,
-		struct kobj_attribute *attr, char *buf)
-{
-	return sprintf(buf, "%u\n", compaction_proactiveness);
-}
-
-COMPACTION_ATTR(proactiveness);
-
-static struct attribute *compaction_attrs[] = {
-	&proactiveness_attr.attr,
-	NULL,
-};
-
-static const struct attribute_group compaction_attr_group = {
-	.attrs = compaction_attrs,
-};
-
-static void __init compaction_sysfs_init(void)
-{
-	compaction_kobj = kobject_create_and_add("compaction", mm_kobj);
-	if (!compaction_kobj)
-		return;
-
-	if (sysfs_create_group(compaction_kobj, &compaction_attr_group)) {
-		kobject_put(compaction_kobj);
-		compaction_kobj = NULL;
-	}
-}
-#endif
-
 static inline bool kcompactd_work_requested(pg_data_t *pgdat)
 {
 	return pgdat->kcompactd_max_order > 0 || kthread_should_stop();
@@ -2729,8 +2664,6 @@ static void kcompactd_do_work(pg_data_t *pgdat)
 		.mode = MIGRATE_SYNC_LIGHT,
 		.ignore_skip_hint = false,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
-		.proactive_compaction = false,
 	};
 	trace_mm_compaction_kcompactd_wake(pgdat->node_id, cc.order,
 							cc.classzone_idx);
@@ -2948,8 +2881,6 @@ static int __init kcompactd_init(void)
 		return ret;
 	}
 
-	compaction_sysfs_init();
-
 	for_each_node_state(nid, N_MEMORY)
 		kcompactd_run(nid);
 	return 0;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 65809fccb..c542eb07b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -8411,7 +8411,6 @@ int alloc_contig_range(unsigned long start, unsigned long end,
 		.ignore_skip_hint = true,
 		.no_set_skip_hint = true,
 		.gfp_mask = current_gfp_context(gfp_mask),
-		.proactive_compaction = false,
 	};
 	INIT_LIST_HEAD(&cc.migratepages);
 
-- 
2.27.0.rc2

